{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unstructured.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMix1yermz0NlTxV6H8MKoS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/witold87/pub_haystack_unstructurized/blob/develop/unstructured.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install farm-haystack==0.8.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bxFxy9MYg-2",
        "outputId": "ae11aee1-ba13-4e36-b9f5-8a6bb77e06f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting farm-haystack==0.8.0\n",
            "  Downloading farm-haystack-0.8.0.tar.gz (152 kB)\n",
            "\u001b[K     |████████████████████████████████| 152 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting farm==0.7.1\n",
            "  Downloading farm-0.7.1-py3-none-any.whl (203 kB)\n",
            "\u001b[K     |████████████████████████████████| 203 kB 60.5 MB/s \n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.72.0-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.17.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 11.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.8.0) (1.1.5)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.8.0) (0.0)\n",
            "Collecting elasticsearch<=7.10,>=7.7\n",
            "  Downloading elasticsearch-7.10.0-py2.py3-none-any.whl (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 74.4 MB/s \n",
            "\u001b[?25hCollecting elastic-apm\n",
            "  Downloading elastic_apm-6.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (359 kB)\n",
            "\u001b[K     |████████████████████████████████| 359 kB 73.9 MB/s \n",
            "\u001b[?25hCollecting tox\n",
            "  Downloading tox-3.24.5-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: coverage in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.8.0) (3.7.1)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 51.5 MB/s \n",
            "\u001b[?25hCollecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 52.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.8.0) (1.4.29)\n",
            "Collecting sqlalchemy_utils\n",
            "  Downloading SQLAlchemy_Utils-0.38.2-py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 13.4 MB/s \n",
            "\u001b[?25hCollecting faiss-cpu>=1.6.3\n",
            "  Downloading faiss_cpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 21.3 MB/s \n",
            "\u001b[?25hCollecting tika\n",
            "  Downloading tika-1.24.tar.gz (28 kB)\n",
            "Collecting httptools\n",
            "  Downloading httptools-0.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (402 kB)\n",
            "\u001b[K     |████████████████████████████████| 402 kB 75.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.8.0) (3.2.5)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.8.0) (8.12.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.8.0) (2.6.3)\n",
            "Collecting pymilvus\n",
            "  Downloading pymilvus-1.1.2-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting SPARQLWrapper\n",
            "  Downloading SPARQLWrapper-1.8.5-py3-none-any.whl (26 kB)\n",
            "Collecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 58.7 MB/s \n",
            "\u001b[?25hCollecting uvloop==0.14\n",
            "  Downloading uvloop-0.14.0-cp37-cp37m-manylinux2010_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farm==0.7.1->farm-haystack==0.8.0) (4.62.3)\n",
            "Collecting torch<1.8,>1.5\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n",
            "\u001b[?25hCollecting dotmap\n",
            "  Downloading dotmap-1.3.26-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from farm==0.7.1->farm-haystack==0.8.0) (5.4.8)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from farm==0.7.1->farm-haystack==0.8.0) (1.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from farm==0.7.1->farm-haystack==0.8.0) (57.4.0)\n",
            "Collecting flask-restplus\n",
            "  Downloading flask_restplus-0.13.0-py2.py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 58.4 MB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farm==0.7.1->farm-haystack==0.8.0) (2.23.0)\n",
            "Collecting mlflow<=1.13.1\n",
            "  Downloading mlflow-1.13.1-py3-none-any.whl (14.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.1 MB 32.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from farm==0.7.1->farm-haystack==0.8.0) (1.4.1)\n",
            "Collecting Werkzeug==0.16.1\n",
            "  Downloading Werkzeug-0.16.1-py2.py3-none-any.whl (327 kB)\n",
            "\u001b[K     |████████████████████████████████| 327 kB 78.5 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 61.6 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.20.40-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 59.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from farm==0.7.1->farm-haystack==0.8.0) (0.37.1)\n",
            "Collecting transformers==4.1.1\n",
            "  Downloading transformers-4.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 68.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from farm==0.7.1->farm-haystack==0.8.0) (0.3.4)\n",
            "Collecting flask-cors\n",
            "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.1.1->farm==0.7.1->farm-haystack==0.8.0) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 60.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 60.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.1.1->farm==0.7.1->farm-haystack==0.8.0) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.1.1->farm==0.7.1->farm-haystack==0.8.0) (3.4.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.1.1->farm==0.7.1->farm-haystack==0.8.0) (2019.12.20)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch<=7.10,>=7.7->farm-haystack==0.8.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch<=7.10,>=7.7->farm-haystack==0.8.0) (1.24.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm==0.7.1->farm-haystack==0.8.0) (1.3.0)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm==0.7.1->farm-haystack==0.8.0) (0.4.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm==0.7.1->farm-haystack==0.8.0) (7.1.2)\n",
            "Collecting azure-storage-blob>=12.0.0\n",
            "  Downloading azure_storage_blob-12.9.0-py2.py3-none-any.whl (356 kB)\n",
            "\u001b[K     |████████████████████████████████| 356 kB 83.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm==0.7.1->farm-haystack==0.8.0) (3.13)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm==0.7.1->farm-haystack==0.8.0) (0.3)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm==0.7.1->farm-haystack==0.8.0) (3.17.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm==0.7.1->farm-haystack==0.8.0) (1.15.0)\n",
            "Collecting querystring-parser\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting gitpython>=2.1.0\n",
            "  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 80.3 MB/s \n",
            "\u001b[?25hCollecting docker>=4.0.0\n",
            "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 67.7 MB/s \n",
            "\u001b[?25hCollecting databricks-cli>=0.8.7\n",
            "  Downloading databricks-cli-0.16.2.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting prometheus-flask-exporter\n",
            "  Downloading prometheus_flask_exporter-0.18.7-py3-none-any.whl (17 kB)\n",
            "Collecting alembic<=1.4.1\n",
            "  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 62.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm==0.7.1->farm-haystack==0.8.0) (2.8.2)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting azure-core<2.0.0,>=1.10.0\n",
            "  Downloading azure_core-1.21.1-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 78.8 MB/s \n",
            "\u001b[?25hCollecting msrest>=0.6.21\n",
            "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.1.4\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 58.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.7.1->farm-haystack==0.8.0) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.7.1->farm-haystack==0.8.0) (2.21)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow<=1.13.1->farm==0.7.1->farm-haystack==0.8.0) (0.8.9)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow<=1.13.1->farm==0.7.1->farm-haystack==0.8.0) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting isodate>=0.6.0\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 889 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.7.1->farm-haystack==0.8.0) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.7.1->farm-haystack==0.8.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.7.1->farm-haystack==0.8.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.7.1->farm-haystack==0.8.0) (3.1.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.4.2->farm-haystack==0.8.0) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.4.2->farm-haystack==0.8.0) (4.10.0)\n",
            "Collecting botocore<1.24.0,>=1.23.40\n",
            "  Downloading botocore-1.23.40-py3-none-any.whl (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 54.9 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 11.4 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting urllib3<2,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 73.5 MB/s \n",
            "\u001b[?25hCollecting starlette==0.17.1\n",
            "  Downloading starlette-0.17.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n",
            "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 63.4 MB/s \n",
            "\u001b[?25hCollecting anyio<4,>=3.0.0\n",
            "  Downloading anyio-3.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 11.6 MB/s \n",
            "\u001b[?25hCollecting sniffio>=1.1\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.7.1->farm-haystack==0.8.0) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.7.1->farm-haystack==0.8.0) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->farm==0.7.1->farm-haystack==0.8.0) (2.0.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.7.1->farm-haystack==0.8.0) (2018.9)\n",
            "Collecting aniso8601>=0.82\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.7.1->farm-haystack==0.8.0) (4.3.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.4.2->farm-haystack==0.8.0) (3.7.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->flask-restplus->farm==0.7.1->farm-haystack==0.8.0) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->flask-restplus->farm==0.7.1->farm-haystack==0.8.0) (5.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->flask-restplus->farm==0.7.1->farm-haystack==0.8.0) (0.18.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.1.1->farm==0.7.1->farm-haystack==0.8.0) (3.0.6)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow<=1.13.1->farm==0.7.1->farm-haystack==0.8.0) (0.12.0)\n",
            "Collecting ujson>=2.0.0\n",
            "  Downloading ujson-5.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting grpcio<1.38.0,>=1.22.0\n",
            "  Downloading grpcio-1.37.1-cp37-cp37m-manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 72.7 MB/s \n",
            "\u001b[?25hCollecting grpcio-tools<1.38.0,>=1.22.0\n",
            "  Downloading grpcio_tools-1.37.1-cp37-cp37m-manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 54.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx->farm-haystack==0.8.0) (4.2.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.1.1->farm==0.7.1->farm-haystack==0.8.0) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->farm==0.7.1->farm-haystack==0.8.0) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->farm==0.7.1->farm-haystack==0.8.0) (3.0.0)\n",
            "Collecting rdflib>=4.0\n",
            "  Downloading rdflib-6.1.1-py3-none-any.whl (482 kB)\n",
            "\u001b[K     |████████████████████████████████| 482 kB 79.5 MB/s \n",
            "\u001b[?25hCollecting pluggy>=0.12.0\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0\n",
            "  Downloading virtualenv-20.13.0-py2.py3-none-any.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 53.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.9.4 in /usr/local/lib/python3.7/dist-packages (from tox->farm-haystack==0.8.0) (0.10.2)\n",
            "Requirement already satisfied: py>=1.4.17 in /usr/local/lib/python3.7/dist-packages (from tox->farm-haystack==0.8.0) (1.11.0)\n",
            "Collecting distlib<1,>=0.3.1\n",
            "  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n",
            "\u001b[K     |████████████████████████████████| 461 kB 78.6 MB/s \n",
            "\u001b[?25hCollecting platformdirs<3,>=2\n",
            "  Downloading platformdirs-2.4.1-py3-none-any.whl (14 kB)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting asgiref>=3.4.0\n",
            "  Downloading asgiref-3.4.1-py3-none-any.whl (25 kB)\n",
            "Building wheels for collected packages: farm-haystack, alembic, databricks-cli, langdetect, python-docx, python-multipart, seqeval, tika\n",
            "  Building wheel for farm-haystack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for farm-haystack: filename=farm_haystack-0.8.0-py3-none-any.whl size=148997 sha256=241b63ad4a5b8bbb7bc1c23d0fe81bc9d67a180051348daf3a33755f71044160\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/e4/43/3780ce56b6422c756c468f0a3c76487026ef8fdf768a386153\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158171 sha256=fafc9b4f23c26c7e299b5fd70fc287802be1105982378ae99bf4e6d730643475\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/5d/0a/9e13f53f4f5dfb67cd8d245bb7cdffe12f135846f491a283e3\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.16.2-py3-none-any.whl size=106811 sha256=31c64d8d42b072dc853179ead79088d0f154393e5c14d8675c97dff6f89d4e09\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/5c/ed/e1ce20a53095f63b27b4964abbad03e59cf3472822addf7d29\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=6054871408384bf05187c7011539571f494a80c15e940701d0e9065c862ee661\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=0cb0628cddce74e29d7f9635fa5e12f1f18c43a1e054fca45a48e5fd07a507f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=02d9a5f8f96d60351b95e1960658c32569d4164914ccab0e154e809fa2a5dc97\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=83d8d9d7721edb36392faf40c86655aa2a3e1d60cc632f444e98b3c7de3714af\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32893 sha256=2f8e6fd76caa4600f2e7b3142ad5204db2a1b33ec08caca22493e73545131329\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/2b/38/58ff05467a742e32f67f5d0de048fa046e764e2fbb25ac93f3\n",
            "Successfully built farm-haystack alembic databricks-cli langdetect python-docx python-multipart seqeval tika\n",
            "Installing collected packages: urllib3, Werkzeug, smmap, jmespath, isodate, websocket-client, sniffio, python-editor, msrest, Mako, gitdb, cryptography, botocore, azure-core, tokenizers, sacremoses, s3transfer, querystring-parser, prometheus-flask-exporter, platformdirs, gunicorn, grpcio, gitpython, docker, distlib, databricks-cli, azure-storage-blob, anyio, aniso8601, alembic, virtualenv, ujson, transformers, torch, starlette, seqeval, sentencepiece, rdflib, pydantic, pluggy, mlflow, h11, grpcio-tools, flask-restplus, flask-cors, dotmap, boto3, asgiref, uvloop, uvicorn, tox, tika, sqlalchemy-utils, SPARQLWrapper, python-multipart, python-docx, pymilvus, psycopg2-binary, langdetect, httptools, fastapi, farm, faiss-cpu, elasticsearch, elastic-apm, farm-haystack\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.43.0\n",
            "    Uninstalling grpcio-1.43.0:\n",
            "      Successfully uninstalled grpcio-1.43.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
            "pytest 3.6.4 requires pluggy<0.8,>=0.5, but you have pluggy 1.0.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed Mako-1.1.6 SPARQLWrapper-1.8.5 Werkzeug-0.16.1 alembic-1.4.1 aniso8601-9.0.1 anyio-3.5.0 asgiref-3.4.1 azure-core-1.21.1 azure-storage-blob-12.9.0 boto3-1.20.40 botocore-1.23.40 cryptography-36.0.1 databricks-cli-0.16.2 distlib-0.3.4 docker-5.0.3 dotmap-1.3.26 elastic-apm-6.7.2 elasticsearch-7.10.0 faiss-cpu-1.7.2 farm-0.7.1 farm-haystack-0.8.0 fastapi-0.72.0 flask-cors-3.0.10 flask-restplus-0.13.0 gitdb-4.0.9 gitpython-3.1.26 grpcio-1.37.1 grpcio-tools-1.37.1 gunicorn-20.1.0 h11-0.13.0 httptools-0.3.0 isodate-0.6.1 jmespath-0.10.0 langdetect-1.0.9 mlflow-1.13.1 msrest-0.6.21 platformdirs-2.4.1 pluggy-1.0.0 prometheus-flask-exporter-0.18.7 psycopg2-binary-2.9.3 pydantic-1.9.0 pymilvus-1.1.2 python-docx-0.8.11 python-editor-1.0.4 python-multipart-0.0.5 querystring-parser-1.2.4 rdflib-6.1.1 s3transfer-0.5.0 sacremoses-0.0.47 sentencepiece-0.1.96 seqeval-1.2.2 smmap-5.0.0 sniffio-1.2.0 sqlalchemy-utils-0.38.2 starlette-0.17.1 tika-1.24 tokenizers-0.9.4 torch-1.7.1 tox-3.24.5 transformers-4.1.1 ujson-5.1.0 urllib3-1.25.11 uvicorn-0.17.0 uvloop-0.14.0 virtualenv-20.13.0 websocket-client-1.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "from haystack.document_store.memory import InMemoryDocumentStore\n",
        "from haystack.document_store.faiss import FAISSDocumentStore\n",
        "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
        "#from haystack.preprocessor.utils import convert_files_to_dicts, fetch_archive_from_http\n",
        "from haystack.reader import TransformersReader\n",
        "from haystack.reader import FARMReader\n",
        "from haystack.retriever import DensePassageRetriever\n",
        "from haystack.retriever import ElasticsearchRetriever\n",
        "from haystack.utils import print_answers, launch_es\n",
        "from haystack.preprocessor.cleaning import clean_wiki_text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yaMxhTp-laI",
        "outputId": "55fe2d92-8ccf-47b0-e474-4d6e82bb475b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "01/21/2022 10:10:09 - INFO - faiss.loader -   Loading faiss with AVX2 support.\n",
            "01/21/2022 10:10:09 - INFO - faiss.loader -   Could not load library with AVX2 support due to:\n",
            "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
            "01/21/2022 10:10:09 - INFO - faiss.loader -   Loading faiss.\n",
            "01/21/2022 10:10:09 - INFO - faiss.loader -   Successfully loaded faiss.\n",
            "01/21/2022 10:10:10 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reader = FARMReader(model_name_or_path=\"deepset/xlm-roberta-large-squad2\", use_gpu=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "745htnj5YFsC",
        "outputId": "3181597e-5ea8-445c-c133-05a2355bdfe2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "01/21/2022 10:10:15 - INFO - farm.utils -   Using device: CPU \n",
            "01/21/2022 10:10:15 - INFO - farm.utils -   Number of GPUs: 0\n",
            "01/21/2022 10:10:15 - INFO - farm.utils -   Distributed Training: False\n",
            "01/21/2022 10:10:15 - INFO - farm.utils -   Automatic Mixed Precision: None\n",
            "01/21/2022 10:11:14 - WARNING - farm.utils -   ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
            "01/21/2022 10:11:14 - INFO - farm.utils -   Using device: CPU \n",
            "01/21/2022 10:11:14 - INFO - farm.utils -   Number of GPUs: 0\n",
            "01/21/2022 10:11:14 - INFO - farm.utils -   Distributed Training: False\n",
            "01/21/2022 10:11:14 - INFO - farm.utils -   Automatic Mixed Precision: None\n",
            "01/21/2022 10:11:15 - INFO - farm.infer -   Got ya 2 parallel workers to do inference ...\n",
            "01/21/2022 10:11:15 - INFO - farm.infer -    0    0 \n",
            "01/21/2022 10:11:15 - INFO - farm.infer -   /w\\  /w\\\n",
            "01/21/2022 10:11:15 - INFO - farm.infer -   /'\\  / \\\n",
            "01/21/2022 10:11:15 - INFO - farm.infer -     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAjQhGUTRmwk",
        "outputId": "2fa44cb6-b0ea-4d56-8e5a-192d8c29b8e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vorwort\n",
            "\n",
            "Der Halbjahresbericht enthält bestimmte finanzielle\n",
            "\n",
            "Leistungskennzahlen, die nicht nach IFRS definiert sind und\n",
            "\n",
            "die von der Geschäftsleitung dazu verwendet werden, die fi­\n",
            "\n",
            "nanzielle und die operative Leistung der Gruppe zu bewerten.\n",
            "\n",
            "Dazu zählen unter anderem:\n",
            "\n",
            "– das organische Wachstum, das interne Realwachstum\n",
            "\n",
            "und Preisanpassungen;\n",
            "\n",
            "– die zugrunde liegende operative Ergebnismarge und die\n",
            "\n",
            "operative Ergebnismarge;\n",
            "\n",
            "– die Nettoverschuldung;\n",
            "\n",
            "– der freie Geldfluss; und\n",
            "\n",
            "– der zugrunde liegende Gewinn je Aktie (Earnings per\n",
            "\n",
            "Share EPS) und der EPS bei konstanten Wechselkursen.\n",
            "\n",
            "Die Geschäftsleitung geht davon aus, dass diese nicht nach\n",
            "\n",
            "IFRS definierten finanziellen Leistungskennzahlen hilfreiche\n",
            "\n",
            "Informationen zur finanziellen und operativen Leistung der\n",
            "\n",
            "Gruppe liefern.\n",
            "\n",
            "Im Dokument «Alternative Leistungskennzahlen», das unter\n",
            "\n",
            "www.nestle.com/investors/publications veröffentlicht ist,\n",
            "\n",
            "werden diese nicht nach IFRS definierten finanziellen\n",
            "\n",
            "Leistungskennzahlen definiert.\n",
            "\n",
            "Halbjahresbericht der Nestlé-Gruppe 20212\n",
            "\n",
            "Lockerung der Mobilitätseinschränkungen in einigen\n",
            "\n",
            "Regionen unterstützt und belief sich auf 21,3%.\n",
            "\n",
            "Nettoveräusserungen reduzierten den Umsatz um 3,1%.\n",
            "\n",
            "Den Hauptanteil daran hatten die Veräusserungen der\n",
            "\n",
            "Marken von Nestlé Waters Nordamerika, von Herta\n",
            "\n",
            "Charcuterie (Aufschnitt und Fleischwaren) und des Yinlu­\n",
            "\n",
            "Geschäfts mit Erdnussmilch und Reisporridge­Konserven.\n",
            "\n",
            "Wechselkurseffekte schmälerten den Umsatz um 3,5%,\n",
            "\n",
            "bedingt durch die Aufwertung des Schweizer Frankens\n",
            "\n",
            "gegenüber den meisten Währungen. Der publizierte Umsatz\n",
            "\n",
            "stieg um 1,5% auf CHF 41,8 Milliarden.\n",
            "\n",
            "Zugrunde liegendes operatives Ergebnis\n",
            "\n",
            "Das zugrunde liegende operative Ergebnis stieg um 1,3%\n",
            "\n",
            "auf CHF 7,3 Milliarden. Die zugrunde liegende operative\n",
            "\n",
            "Ergebnismarge zu konstanten Wechselkursen und auf publi­\n",
            "\n",
            "zierter Basis blieb mit 17,4% auf Vorjahresniveau.\n",
            "\n",
            "Die Bruttomarge stieg um 20 Basispunkte auf 48,8%. Die\n",
            "\n",
            "Ausgaben für direktes Konsumentenmarketing * stiegen um\n",
            "\n",
            "80 Basispunkte über das Niveau von 2019, nachdem 2020\n",
            "\n",
            "weniger In­Store­Aktivierungen stattgefunden hatten. Auch\n",
            "\n",
            "die Kosteninflation beeinträchtigte im zweiten Quartal die\n",
            "\n",
            "Margenentwicklung. Diese Erhöhungen neutralisierten den\n",
            "\n",
            "Operating Leverage, die strukturellen Kostenreduktionen,\n",
            "\n",
            "Preisanpassungen und geringere Kosten im Zusammenhang\n",
            "\n",
            "mit COVID­19.\n",
            "\n",
            "Restrukturierungsausgaben und sonstige Nettoaufwen­\n",
            "\n",
            "dungen stiegen um CHF 78 Millionen auf CHF 264 Millionen.\n",
            "\n",
            "Grund hierfür waren höhere Wertminderungen von Anlage­\n",
            "\n",
            "vermögen. Das operative Ergebnis erhöhte sich um 0,2% auf\n",
            "\n",
            "CHF 7,0 Milliarden. Die zugrunde liegende operative Ergebnis­\n",
            "\n",
            "marge belief sich auf 16,7%. Dies entspricht zu konstanten\n",
            "\n",
            "Wechselkursen und auf einer publizierten Basis einem\n",
            "\n",
            "Rückgang von 20 Basispunkten.\n",
            "\n",
            "Nettofinanzaufwand und Ertragssteuern\n",
            "\n",
            "Der Nettofinanzaufwand sank bedingt durch niedrigere\n",
            "\n",
            "Fremdkapitalkosten um 6,9% auf CHF 416 Millionen.\n",
            "\n",
            "Der publizierte Steuersatz der Gruppe sank aufgrund von\n",
            "\n",
            "Sonderposten um 970 Basispunkte auf 17,4%. Der zugrunde\n",
            "\n",
            "liegende Steuersatz sank um 120 Basispunkte auf 20,2%,\n",
            "\n",
            "hauptsächlich bedingt durch die geografische Ausrichtung\n",
            "\n",
            "und den Geschäftsmix.\n",
            "\n",
            "Resultate der Gruppe\n",
            "\n",
            "Umsatz der Gruppe\n",
            "\n",
            "Das organische Wachstum erreichte 8,1% und das interne\n",
            "\n",
            "Realwachstum betrug 6,8%. Die Preisanpassungen stiegen\n",
            "\n",
            "auf 1,3%, was den Anstieg der Einkaufspreise widerspiegelte.\n",
            "\n",
            "Das Wachstum war in den meisten Regionen breit abge­\n",
            "\n",
            "stützt. Das organische Wachstum in den Industrieländern\n",
            "\n",
            "betrug 6,7% und beruhte meistens auf internem Realwachs­\n",
            "\n",
            "tum. Das organische Wachstum in den aufstrebenden\n",
            "\n",
            "Märkten belief sich auf 10,0%, mit starkem internem Real­\n",
            "\n",
            "wachstum und positiven Preisanpassungen.\n",
            "\n",
            "Nach Produktkategorien leistete Kaffee den grössten\n",
            "\n",
            "Wachstumsbeitrag, insbesondere dank der hohen Nachfrage\n",
            "\n",
            "nach den drei Hauptmarken Nescafé, Nespresso und\n",
            "\n",
            "Starbucks. Starbucks­Produkte verzeichneten ein Wachstum\n",
            "\n",
            "von 16,7% und der Umsatz kletterte auf CHF 1,4 Milliarden in\n",
            "\n",
            "79 Märkten. Purina­Produkte für Heimtiere erzielten ein zwei­\n",
            "\n",
            "stelliges Wachstum, angeführt von den wissenschaftsba­\n",
            "\n",
            "sierten und Premiummarken Purina Pro Plan, Purina ONE und\n",
            "\n",
            "Felix sowie von den Veterinärprodukten. Fertiggerichte und\n",
            "\n",
            "Kulinarikprodukte profitierten von einer starken Nachfrage\n",
            "\n",
            "nach Maggi und Stouffer’s und verzeichneten ein hohes ein­\n",
            "\n",
            "stelliges Wachstum. Vegetarische und pflanzenbasierte\n",
            "\n",
            "Produkte verzeichneten ein starkes zweistelliges Wachstum,\n",
            "\n",
            "angeführt von Garden Gourmet. Milchprodukte wiesen ein\n",
            "\n",
            "hohes einstelliges Wachstum aus, das von angereicherter\n",
            "\n",
            "Milch, Kaffeeweisser und Speiseeis getragen wurde. Das\n",
            "\n",
            "Süsswarengeschäft wuchs zweistellig, unterstützt durch die\n",
            "\n",
            "kräftige Umsatzentwicklung bei Impulskäufen. Der Umsatz\n",
            "\n",
            "von Nestlé Health Science verzeichnete ein zweistelliges\n",
            "\n",
            "Wachstum, das die hohe Nachfrage nach Vitaminen,\n",
            "\n",
            "Mineralstoffen und Nahrungsergänzungsmitteln sowie nach\n",
            "\n",
            "Produkten für gesundes Altern widerspiegelt. Säuglings­ und\n",
            "\n",
            "Babynahrung verbuchte einen Umsatzrückgang, der auf rück­\n",
            "\n",
            "läufige Geburtenraten im Zusammenhang mit der Pandemie\n",
            "\n",
            "zurückzuführen war. Wasserprodukte kehrten zu positivem\n",
            "\n",
            "Wachstum zurück, das von den internationalen Premium­\n",
            "\n",
            "marken S.Pellegrino und Perrier getragen wurde.\n",
            "\n",
            "Bei den Verkaufskanälen verzeichneten die Einzelhandels­\n",
            "\n",
            "umsätze ein organisches Wachstum von 7,3%, das sich im\n",
            "\n",
            "zweiten Quartal aufgrund der hohen Vergleichsbasis 2020 auf\n",
            "\n",
            "einen mittleren einstelligen Zuwachs verlangsamte. Verkäufe\n",
            "\n",
            "im E­Commerce legten um 19,2% zu und erreichten 14,6% des\n",
            "\n",
            "Gesamtumsatzes der Gruppe. Die Dynamik war in den meis­\n",
            "\n",
            "ten Kategorien stark, besonders bei Kaffee, Purina­Produkten\n",
            "\n",
            "für Heimtiere und Kulinarikprodukten. Das organische\n",
            "\n",
            "Wachstum in den Ausser­Haus­Kanälen wurde von der\n",
            "\n",
            "Brief an unsere Aktionäre\n",
            "\n",
            "* Zahlen 2019 ohne Nestlé Skin Health\n",
            "\n",
            "Halbjahresbericht der Nestlé-Gruppe 20213\n",
            "\n",
            "Reingewinn und Gewinn je Aktie\n",
            "\n",
            "Der Reingewinn stieg um 1,1% auf CHF 5,9 Milliarden. Die\n",
            "\n",
            "Nettoreingewinnmarge sank aufgrund von ausserordentlichen\n",
            "\n",
            "Erträgen aus Veräusserungen im Jahr 2020 um 10 Basispunkte\n",
            "\n",
            "auf 14,2%.\n",
            "\n",
            "Der zugrunde liegende Gewinn je Aktie stieg zu konstanten\n",
            "\n",
            "Wechselkursen um 10,5% und auf einer publizierten Basis um\n",
            "\n",
            "8,3% auf CHF 2.17. Dieser Anstieg war hauptsächlich auf das\n",
            "\n",
            "verbesserte operative Ergebnis zurückzuführen. Das Aktien­\n",
            "\n",
            "rückkaufprogramm von Nestlé trug – abzüglich des Finanzie­\n",
            "\n",
            "rungsaufwands – 1,4% zur Zunahme des zugrunde liegenden\n",
            "\n",
            "Gewinns je Aktie bei. Der publizierte Gewinn je Aktie stieg um\n",
            "\n",
            "3,2% auf CHF 2.12.\n",
            "\n",
            "Geldfluss\n",
            "\n",
            "Der operative Geldfluss blieb nahezu unverändert bei\n",
            "\n",
            "CHF 5,8 Milliarden. Der freie Geldfluss sank von CHF 3,3 Mil­\n",
            "\n",
            "liarden auf CHF 2,8 Milliarden. Dies widerspiegelt haupt­\n",
            "\n",
            "sächlich den vorübergehend gestiegenen Investitionsaufwand\n",
            "\n",
            "zur Befriedigung der starken Konsumentennachfrage, insbe­\n",
            "\n",
            "sondere nach Purina­Produkten für Heimtiere und Kaffee.\n",
            "\n",
            "Aktienrückkaufprogramm\n",
            "\n",
            "Die Gruppe kaufte im ersten Halbjahr Nestlé­Aktien im Wert\n",
            "\n",
            "von CHF 3,1 Milliarden zurück, dies im Rahmen des im Januar\n",
            "\n",
            "2020 lancierten und auf drei Jahre ausgelegten Aktienrück­\n",
            "\n",
            "kaufprogramms über CHF 20 Milliarden.\n",
            "\n",
            "Nettoverschuldung\n",
            "\n",
            "Die Nettoverschuldung stieg per 30. Juni 2021 auf\n",
            "\n",
            "CHF 38,5 Milliarden, verglichen mit CHF 31,3 Milliarden am\n",
            "\n",
            "31. Dezember 2020. Der Anstieg widerspiegelt die Dividenden­\n",
            "\n",
            "ausschüttung in Höhe von CHF 7,7 Milliarden und die Aktien­\n",
            "\n",
            "rückkäufe im Wert von CHF 3,1 Milliarden, welche den freien\n",
            "\n",
            "Geldfluss und den Nettozufluss aus Veräusserungen und\n",
            "\n",
            "Akquisitionen mehr als neutralisierten.\n",
            "\n",
            "Portfoliomanagement\n",
            "\n",
            "Nestlé richtet das globale Wassergeschäft neu aus und ver­\n",
            "\n",
            "stärkt den Fokus auf internationale Premium­ und Mineral­\n",
            "\n",
            "wassermarken sowie auf Produkte für eine gesunde Flüssig­\n",
            "\n",
            "keitsaufnahme. Am 5. März 2021 schloss Nestlé die Akquisition\n",
            "\n",
            "von Essentia Water ab, einer Premiummarke für funktionelle\n",
            "\n",
            "Wasserprodukte in den Vereinigten Staaten von Amerika.\n",
            "\n",
            "Am 31. März 2021 schloss Nestlé den Verkauf ihrer regionalen\n",
            "\n",
            "Quellwassermarken, des Geschäfts mit aufbereitetem\n",
            "\n",
            "Flaschenwasser sowie des Getränkelieferservices in den\n",
            "\n",
            "Vereinigten Staaten von Amerika und Kanada für\n",
            "\n",
            "USD 4,3 Milliarden ab.\n",
            "\n",
            "Nestlé Health Science konzentriert sich weiterhin auf den\n",
            "\n",
            "Ausbau der Führungsposition im Bereich Ernährung und\n",
            "\n",
            "Gesundheit. Am 30. April 2021 traf Nestlé eine Vereinbarung\n",
            "\n",
            "zum Kauf der Kernmarken von The Bountiful Company für\n",
            "\n",
            "USD 5,75 Milliarden. The Bountiful Company ist der weltweit\n",
            "\n",
            "führende, fokussierte Anbieter in der sehr attraktiven und\n",
            "\n",
            "wachsenden Kategorie Ernährung und Nahrungsergänzungs­\n",
            "\n",
            "mittel. Der Abschluss der Transaktion wird im August erwartet.\n",
            "\n",
            "Am 1. Juli 2021 vollendete Nestlé die Akquisition von Nuun,\n",
            "\n",
            "einem führenden Anbieter im rasch wachsenden Markt für\n",
            "\n",
            "funktionelle Flüssigkeitsaufnahme. Am 1. Juli 2021 schloss\n",
            "\n",
            "Nestlé Health Science mit Seres Therapeutics eine Verein­\n",
            "\n",
            "barung zur gemeinsamen Vermarktung von SER­109, einem\n",
            "\n",
            "Produktkandidaten für ein orales Mikrobiom­Therapeutikum\n",
            "\n",
            "in den Vereinigten Staaten von Amerika und Kanada. Bei einer\n",
            "\n",
            "Zulassung wäre SER­109 das erste jemals von der US­Gesund­\n",
            "\n",
            "heitsbehörde FDA zugelassene Mikrobiom­Therapeutikum.\n",
            "\n",
            "Aufbauend auf dem Erfolg der Global Coffee Alliance, er­\n",
            "\n",
            "höht Nestlé die globale Reichweite von Kaffee­ und Tee­\n",
            "\n",
            "produkten der Marke Starbucks ausserhalb der Starbucks­\n",
            "\n",
            "Filialen. Am 26. Juli 2021 gaben Nestlé und Starbucks eine\n",
            "\n",
            "neue Kooperation zur Einführung von trinkfertigen Starbucks­\n",
            "\n",
            "Kaffeegetränken in ausgewählten Märkten in Südostasien,\n",
            "\n",
            "Ozeanien und Lateinamerika beka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/haystack/document_store/sql.py:294: SAWarning: relationship 'DocumentORM.Meta' will copy column document.id to column meta.document_id, which conflicts with relationship(s): 'DocumentORM.meta' (copies document.id to meta.document_id), 'MetaORM.Document' (copies document.id to meta.document_id). If this is not the intention, consider if these relationships should be linked with back_populates, or if viewonly=True should be applied to one or more if they are read-only. For the less common case that foreign key constraints are partially overlapping, the orm.foreign() annotation can be used to isolate the columns that should be written towards.   To silence this warning, add the parameter 'overlaps=\"Document,meta\"' to the 'DocumentORM.Meta' relationship. (Background on this error at: https://sqlalche.me/e/14/qzyx)\n",
            "  meta_orms = [MetaORM(name=key, value=value) for key, value in meta_fields.items()]\n",
            "/usr/local/lib/python3.7/dist-packages/haystack/document_store/sql.py:294: SAWarning: relationship 'MetaORM.documents' will copy column document.id to column meta.document_id, which conflicts with relationship(s): 'DocumentORM.meta' (copies document.id to meta.document_id), 'MetaORM.Document' (copies document.id to meta.document_id). If this is not the intention, consider if these relationships should be linked with back_populates, or if viewonly=True should be applied to one or more if they are read-only. For the less common case that foreign key constraints are partially overlapping, the orm.foreign() annotation can be used to isolate the columns that should be written towards.   To silence this warning, add the parameter 'overlaps=\"Document,meta\"' to the 'MetaORM.documents' relationship. (Background on this error at: https://sqlalche.me/e/14/qzyx)\n",
            "  meta_orms = [MetaORM(name=key, value=value) for key, value in meta_fields.items()]\n",
            "01/21/2022 10:11:48 - INFO - haystack.document_store.faiss -   Updating embeddings for 1 docs...\n",
            "10000it [00:00, 10647.28it/s]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:40<00:00, 40.42s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the net debt?\n",
            "CHF 38,5 Milliarden\n",
            "0.9298259615898132\n",
            "overschuldung\n",
            "\n",
            "Die Nettoverschuldung stieg per 30. Juni 2021 auf\n",
            "\n",
            "CHF 38,5 Milliarden, verglichen mit CHF 31,3 Milliarden am\n",
            "\n",
            "31. Dezember 2020. Der A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:40<00:00, 40.27s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wie hoch ist der Umsatz in der Gruppe?\n",
            "CHF 1,4 Milliarden\n",
            "0.1571597009897232\n",
            "erzeichneten ein Wachstum\n",
            "\n",
            "von 16,7% und der Umsatz kletterte auf CHF 1,4 Milliarden in\n",
            "\n",
            "79 Märkten. Purina­Produkte für Heimtiere erzielten ein zwei­\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:39<00:00, 39.78s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the revenue in group?\n",
            "Umsatz der Gruppe\n",
            "\n",
            "Das organische Wachstum erreichte 8,1%\n",
            "0.03432765230536461\n",
            "\n",
            "\n",
            "und den Geschäftsmix.\n",
            "\n",
            "Resultate der Gruppe\n",
            "\n",
            "Umsatz der Gruppe\n",
            "\n",
            "Das organische Wachstum erreichte 8,1% und das interne\n",
            "\n",
            "Realwachstum betrug 6,8%. Di\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:40<00:00, 40.02s/ Batches]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the net profit?\n",
            "CHF 7,0 Milliarden\n",
            "0.35945063829421997\n",
            "lage­\n",
            "\n",
            "vermögen. Das operative Ergebnis erhöhte sich um 0,2% auf\n",
            "\n",
            "CHF 7,0 Milliarden. Die zugrunde liegende operative Ergebnis­\n",
            "\n",
            "marge belief sich auf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ## Task: Question Answering for Game of Thrones\n",
        "#\n",
        "# Question Answering can be used in a variety of use cases. A very common one:  Using it to navigate through complex\n",
        "# knowledge bases or long documents (\"search setting\").\n",
        "#\n",
        "# A \"knowledge base\" could for example be your website, an internal wiki or a collection of financial reports.\n",
        "# In this tutorial we will work on a slightly different domain: \"Game of Thrones\".\n",
        "#\n",
        "# Let's see how we can use a bunch of Wikipedia articles to answer a variety of questions about the\n",
        "# marvellous seven kingdoms.\n",
        "\n",
        "import logging\n",
        "\n",
        "from haystack.document_store.memory import InMemoryDocumentStore\n",
        "from haystack.document_store.faiss import FAISSDocumentStore\n",
        "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
        "#from haystack.preprocessor.utils import convert_files_to_dicts, fetch_archive_from_http\n",
        "from haystack.reader import TransformersReader\n",
        "from haystack.reader import FARMReader\n",
        "from haystack.retriever import DensePassageRetriever\n",
        "from haystack.retriever import ElasticsearchRetriever\n",
        "from haystack.utils import print_answers, launch_es\n",
        "from haystack.preprocessor.cleaning import clean_wiki_text\n",
        "#from memory_profiler import profile\n",
        "\n",
        "def tutorial1_basic_qa_pipeline():\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    # ## Document Store\n",
        "    #\n",
        "    # Haystack finds answers to queries within the documents stored in a `DocumentStore`. The current implementations of\n",
        "    # `DocumentStore` include `ElasticsearchDocumentStore`, `FAISSDocumentStore`, `SQLDocumentStore`, and `InMemoryDocumentStore`.\n",
        "    #\n",
        "    # **Here:** We recommended Elasticsearch as it comes preloaded with features like full-text queries, BM25 retrieval,\n",
        "    # and vector storage for text embeddings.\n",
        "    # **Alternatives:** If you are unable to setup an Elasticsearch instance, then follow the Tutorial 3\n",
        "    # for using SQL/InMemory document stores.\n",
        "    # **Hint**:\n",
        "    # This tutorial creates a new document store instance with Wikipedia articles on Game of Thrones. However, you can\n",
        "    # configure Haystack to work with your existing document stores.\n",
        "    #\n",
        "    # Start an Elasticsearch server\n",
        "    # You can start Elasticsearch on your local machine instance using Docker. If Docker is not readily available in\n",
        "    # your environment (e.g. in Colab notebooks), then you can manually download and execute Elasticsearch from source.\n",
        "\n",
        "    #launch_es()\n",
        "\n",
        "    # Connect to Elasticsearch\n",
        "    #document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")\n",
        "    document_store = FAISSDocumentStore(faiss_index_factory_str='Flat')\n",
        "\n",
        "    # ## Preprocessing of documents\n",
        "    #\n",
        "    # Haystack provides a customizable pipeline for:\n",
        "    # - converting files into texts\n",
        "    # - cleaning texts\n",
        "    # - splitting texts\n",
        "    # - writing them to a Document Store\n",
        "\n",
        "    # In this tutorial, we download Wikipedia articles about Game of Thrones, apply a basic cleaning function, and add\n",
        "    # them in Elasticsearch.\n",
        "\n",
        "    # Let's first fetch some documents that we want to query\n",
        "    # Here: 517 Wikipedia articles for Game of Thrones\n",
        "    #doc_dir = \"data/article_txt_got\"\n",
        "    #s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt.zip\"\n",
        "    #fetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n",
        "\n",
        "    # convert files to dicts containing documents that can be indexed to our datastore\n",
        "    # dicts = convert_files_to_dicts(dir_path=doc_dir, split_paragraphs=True)\n",
        "    #dicts = convert_files_to_dicts(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)\n",
        "    # You can optionally supply a cleaning function that is applied to each doc (e.g. to remove footers)\n",
        "    # It must take a str as input, and return a str.\n",
        "    # dicts = [{'text':\n",
        "    #           \"\"\"Lululemon Athletica Inc. (CA) started trading on May 2, 2007, operates in the Consumer Cyclical sector (Apparel Retail industry), has 25,000 full-time employees, and is led by Mr. Calvin McDonald. lululemon athletica inc., together with its subsidiaries, designs, distributes, and retails athletic apparel and accessories for women and men. It operates through two segments, Company-Operated Stores and Direct to Consumer. The company offers pants, shorts, tops, and jackets for healthy lifestyle and athletic activities, such as yoga, running, and training, as well as other sweaty pursuits. It also provides fitness-related accessories. The company sells its products through a chain of company-operated stores; outlets and warehouse sales; a network of wholesale accounts, such as yoga studios, health clubs, and fitness centers; temporary locations, including seasonal stores; and license and supply arrangements, as well as directly to consumer through mobile apps, and lululemon.com e-commerce website. As of January 31, 2021, it operated 521 company-operated stores under the lululemon brand in the United States, Canada, the People's Republic of China, Australia, the United Kingdom, Japan, New Zealand, Germany, South Korea, Singapore, France, Malaysia, Sweden, Ireland, the Netherlands, Norway, and Switzerland. lululemon athletica inc. was founded in 1998 and is based in Vancouver, Canada.\n",
        "    #           \"\"\",\n",
        "    #           'meta': {'name': 'lulu.txt'}}]\n",
        "    # print(f'{dicts[0]}')\n",
        "\n",
        "    with open('nestle_report_de.txt', 'r') as file:\n",
        "        text = file.readlines()\n",
        "\n",
        "    text_str = '\\n'.join([str(elem) for elem in text])\n",
        "    stripped_text = text_str.strip('\\n')\n",
        "    print(stripped_text)\n",
        "\n",
        "    dicts = [{'text': stripped_text,\n",
        "              'meta': {'name': 'thai_air_th.txt'}}]\n",
        "\n",
        "    document_store.write_documents(dicts)\n",
        "\n",
        "    # ## Initalize Retriever & Reader\n",
        "    #\n",
        "    # ### Retriever\n",
        "    #\n",
        "    # Retrievers help narrowing down the scope for the Reader to smaller units of text where a given question\n",
        "    # could be answered.\n",
        "    #\n",
        "    # They use some simple but fast algorithm.\n",
        "    # **Here:** We use Elasticsearch's default BM25 algorithm\n",
        "    # **Alternatives:**\n",
        "    # - Customize the `ElasticsearchRetriever`with custom queries (e.g. boosting) and filters\n",
        "    # - Use `EmbeddingRetriever` to find candidate documents based on the similarity of\n",
        "    #   embeddings (e.g. created via Sentence-BERT)\n",
        "    # - Use `TfidfRetriever` in combination with a SQL or InMemory Document store for simple prototyping and debugging\n",
        "\n",
        "    retriever = DensePassageRetriever(document_store=document_store)\n",
        "\n",
        "    document_store.update_embeddings(retriever)\n",
        "    # Alternative: An in-memory TfidfRetriever based on Pandas dataframes for building quick-prototypes\n",
        "    # with SQLite document store.\n",
        "    #\n",
        "    # from haystack.retriever.tfidf import TfidfRetriever\n",
        "    # retriever = TfidfRetriever(document_store=document_store)\n",
        "\n",
        "    # ### Reader\n",
        "    #\n",
        "    # A Reader scans the texts returned by retrievers in detail and extracts the k best answers. They are based\n",
        "    # on powerful, but slower deep learning models.\n",
        "    #\n",
        "    # Haystack currently supports Readers based on the frameworks FARM and Transformers.\n",
        "    # With both you can either load a local model or one from Hugging Face's model hub (https://huggingface.co/models).\n",
        "    # **Here:** a medium sized RoBERTa QA model using a Reader based on\n",
        "    #           FARM (https://huggingface.co/deepset/roberta-base-squad2)\n",
        "    # **Alternatives (Reader):** TransformersReader (leveraging the `pipeline` of the Transformers package)\n",
        "    # **Alternatives (Models):** e.g. \"distilbert-base-uncased-distilled-squad\" (fast) or\n",
        "    #                            \"deepset/bert-large-uncased-whole-word-masking-squad2\" (good accuracy)\n",
        "    # **Hint:** You can adjust the model to return \"no answer possible\" with the no_ans_boost. Higher values mean\n",
        "    #           the model prefers \"no answer possible\"\n",
        "    #\n",
        "    # #### FARMReader\n",
        "\n",
        "    # Load a  local model or any of the QA models on\n",
        "    # Hugging Face's model hub (https://huggingface.co/models)\n",
        "    #reader = FARMReader(model_name_or_path=\"deepset/xlm-roberta-large-squad2\", use_gpu=True)\n",
        "\n",
        "    # #### TransformersReader\n",
        "\n",
        "    # Alternative:\n",
        "    # reader = TransformersReader(\n",
        "    #     model_name_or_path=\"distilbert-base-uncased-distilled-squad\", tokenizer=\"distilbert-base-uncased\",\n",
        "    #     use_gpu=-1)\n",
        "\n",
        "    # ### Pipeline\n",
        "    # \n",
        "    # With a Haystack `Pipeline` you can stick together your building blocks to a search pipeline.\n",
        "    # Under the hood, `Pipelines` are Directed Acyclic Graphs (DAGs) that you can easily customize for your own use cases.\n",
        "    # To speed things up, Haystack also comes with a few predefined Pipelines. One of them is the `ExtractiveQAPipeline` that combines a retriever and a reader to answer our questions.\n",
        "    # You can learn more about `Pipelines` in the [docs](https://haystack.deepset.ai/docs/latest/pipelinesmd).\n",
        "    from haystack.pipeline import ExtractiveQAPipeline\n",
        "    #from haystack.pipeline import FAQPipeline\n",
        "\n",
        "\n",
        "    pipe = ExtractiveQAPipeline(reader, retriever)\n",
        "\n",
        "    ## Voilà! Ask a question!\n",
        "    # prediction = pipe.run(\n",
        "    #     query=\"Who is the father of Arya Stark?\"\n",
        "    # )\n",
        "\n",
        "    # questions = ['Who is in the board of directors?',\n",
        "    #              'What is the nature of business?',\n",
        "    #              'When the company was started?',\n",
        "    #              'Where the company operates?',\n",
        "    #              'How many workers they have?']\n",
        "\n",
        "\n",
        "    questions_en = ['What is the net debt?',\n",
        "                    'Wie hoch ist der Umsatz in der Gruppe?',\n",
        "                    'What is the revenue in group?',\n",
        "                    'What is the net profit?']\n",
        "\n",
        "\n",
        "    questions_th = ['หนี้สุทธิคืออะไร??',\n",
        "                    'รายได้ในกลุ่มคืออะไร?',\n",
        "                    'กำไรสุทธิคืออะไร?',\n",
        "                    'how does the covid-19 affect the company?']\n",
        "    \n",
        "    questions_de = ['Wie hoch ist die Nettoverschuldung? ',\n",
        "                    'Wie hoch ist der Umsatz in der Gruppe?' ]\n",
        "\n",
        "\n",
        "    questions_fr = ['Quelle est la dette nette?',\n",
        "                    'Quel est le revenu du groupe?',\n",
        "                    'Quel est le chiffre daffaires du groupe? ']\n",
        "\n",
        "\n",
        "    questions_es = ['¿Cuáles son el resultado de la crisis?',\n",
        "                 '¿Cuál es la naturaleza del negocio?',\n",
        "                 '¿Cuándo se fundó la empresa?',\n",
        "                 ]\n",
        "\n",
        "    def predict(questions, pipe):\n",
        "      for question in questions:\n",
        "          prediction = pipe.run(\n",
        "              query=question\n",
        "          )\n",
        "          print(prediction['query'])\n",
        "          print(prediction['answers'][0]['answer'])\n",
        "          print(prediction['answers'][0]['probability'])\n",
        "          print(prediction['answers'][0]['context'])\n",
        "\n",
        "    predict(questions_en, pipe)\n",
        "\n",
        "    #prediction = pipe.run(query=\"Who created the Dothraki vocabulary?\")\n",
        "    #prediction = pipe.run(query=\"Who is the sister of Sansa?\")\n",
        "\n",
        "    # Now you can either print the object directly\n",
        "\n",
        "    # Sample output:    \n",
        "    # {\n",
        "    #     'answers': [ <Answer: answer='Eddard', type='extractive', score=0.9919578731060028, offsets_in_document=[{'start': 608, 'end': 615}], offsets_in_context=[{'start': 72, 'end': 79}], document_id='cc75f739897ecbf8c14657b13dda890e', meta={'name': '454_Music_of_Game_of_Thrones.txt'}}, context='...' >,\n",
        "    #                  <Answer: answer='Ned', type='extractive', score=0.9767240881919861, offsets_in_document=[{'start': 3687, 'end': 3801}], offsets_in_context=[{'start': 18, 'end': 132}], document_id='9acf17ec9083c4022f69eb4a37187080', meta={'name': '454_Music_of_Game_of_Thrones.txt'}}, context='...' >,\n",
        "    #                  ...\n",
        "    #                ]\n",
        "    #     'documents': [ <Document: content_type='text', score=0.8034909798951382, meta={'name': '332_Sansa_Stark.txt'}, embedding=None, id=d1f36ec7170e4c46cde65787fe125dfe', content='\\n===\\'\\'A Game of Thrones\\'\\'===\\nSansa Stark begins the novel by being betrothed to Crown ...'>,\n",
        "    #                    <Document: content_type='text', score=0.8002150354529785, meta={'name': '191_Gendry.txt'}, embedding=None, id='dd4e070a22896afa81748d6510006d2', 'content='\\n===Season 2===\\nGendry travels North with Yoren and other Night's Watch recruits, including Arya ...'>,\n",
        "    #                    ...\n",
        "    #                  ],\n",
        "    #     'no_ans_gap':  11.688868522644043,\n",
        "    #     'node_id': 'Reader',\n",
        "    #     'params': {'Reader': {'top_k': 5}, 'Retriever': {'top_k': 5}},\n",
        "    #     'query': 'Who is the father of Arya Stark?',\n",
        "    #     'root_node': 'Query'\n",
        "    # }\n",
        "\n",
        "    # Note that the documents contained in the above object are the documents filtered by the Retriever from\n",
        "    # the document store. Although the answers were extracted from these documents, it's possible that many\n",
        "    # answers were taken from a single one of them, and that some of the documents were not source of any answer.\n",
        "\n",
        "    # Or use a util to simplify the output\n",
        "    # Change `minimum` to `medium` or `all` to raise the level of detail\n",
        "    # print(\"\\n\\nSimplified output:\\n\")\n",
        "    # print(prediction)\n",
        "    # print_answers(prediction, details=\"minimum\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tutorial1_basic_qa_pipeline()\n",
        "\n",
        "# This Haystack script was made with love by deepset in Berlin, Germany\n",
        "# Haystack: https://github.com/deepset-ai/haystack\n",
        "# deepset: https://deepset.ai/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep farm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHey__Z9ep0s",
        "outputId": "ee79ac2d-4065-48cb-8652-54a0b73f2f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "farm==0.7.1\n",
            "farm-haystack==0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nowa sekcja"
      ],
      "metadata": {
        "id": "EQ31m5YBb9Xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTwbv-g3YbO9",
        "outputId": "58f0cc17-79c8-4360-97ff-69fedae85355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 56\n",
            "drwxr-xr-x 4 root root 4096 Jan 19 10:44 mlruns\n",
            "-rw-r--r-- 1 root root 9570 Jan 19 11:36 nestle_report_de.txt\n",
            "-rw-r--r-- 1 root root 8625 Jan 19 11:36 nestle_report_en.txt\n",
            "-rw-r--r-- 1 root root 9944 Jan 19 11:36 nestle_report_fr.txt\n",
            "drwxr-xr-x 1 root root 4096 Jan  7 14:33 sample_data\n",
            "-rw-r--r-- 1 root root 2831 Jan 19 10:01 spanish_report.txt\n",
            "-rw-r--r-- 1 root root 1653 Jan 19 10:01 telefonica_report_en.txt\n",
            "-rw-r--r-- 1 root root 1892 Jan 19 10:01 telefonica_report_es.txt\n"
          ]
        }
      ]
    }
  ]
}